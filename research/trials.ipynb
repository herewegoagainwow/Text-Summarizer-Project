{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2520d65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun  6 00:09:57 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...    Off | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   43C    P3              17W /  60W |      8MiB /  8188MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2352      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23963afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "CUDA Version: 12.1\n",
      "PyTorch CUDA support: True\n",
      "Number of CUDA devices: 1\n",
      "CUDA device name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is available!\")\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"PyTorch CUDA support: {torch.backends.cuda.is_built()}\")\n",
    "        print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "        print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c06b8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad9c9d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 00:10:09.234258: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-06 00:10:09.268527: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-06 00:10:09.958580: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 14732\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_from_disk, load_metric\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "dataset_samsum = load_from_disk('samsum_dataset')\n",
    "print(dataset_samsum)\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"summarize: \" + dialogue for dialogue in examples[\"dialogue\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=150, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_samsum = dataset_samsum.map(preprocess_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "397a8518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3476de58daf74f22b8cab2f26b9f6359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22098 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5612, 'grad_norm': 2.371866226196289, 'learning_rate': 1.4660602769481401e-05, 'epoch': 0.07}\n",
      "{'loss': 0.4789, 'grad_norm': 1.6517077684402466, 'learning_rate': 1.4321205538962802e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4445, 'grad_norm': 1.1639117002487183, 'learning_rate': 1.3981808308444204e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4267, 'grad_norm': 3.335114002227783, 'learning_rate': 1.3642411077925605e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4342, 'grad_norm': 0.723832905292511, 'learning_rate': 1.3303013847407006e-05, 'epoch': 0.34}\n",
      "{'loss': 0.4095, 'grad_norm': 1.1829761266708374, 'learning_rate': 1.2963616616888406e-05, 'epoch': 0.41}\n",
      "{'loss': 0.4326, 'grad_norm': 1.2201462984085083, 'learning_rate': 1.2624219386369809e-05, 'epoch': 0.48}\n",
      "{'loss': 0.4064, 'grad_norm': 1.0795049667358398, 'learning_rate': 1.228482215585121e-05, 'epoch': 0.54}\n",
      "{'loss': 0.4077, 'grad_norm': 1.0733861923217773, 'learning_rate': 1.194542492533261e-05, 'epoch': 0.61}\n",
      "{'loss': 0.4209, 'grad_norm': 1.1338129043579102, 'learning_rate': 1.1606027694814009e-05, 'epoch': 0.68}\n",
      "{'loss': 0.399, 'grad_norm': 0.8322863578796387, 'learning_rate': 1.1266630464295411e-05, 'epoch': 0.75}\n",
      "{'loss': 0.3948, 'grad_norm': 0.8125709295272827, 'learning_rate': 1.0927233233776812e-05, 'epoch': 0.81}\n",
      "{'loss': 0.3955, 'grad_norm': 0.8978766798973083, 'learning_rate': 1.0587836003258213e-05, 'epoch': 0.88}\n",
      "{'loss': 0.3869, 'grad_norm': 1.1833115816116333, 'learning_rate': 1.0248438772739614e-05, 'epoch': 0.95}\n",
      "{'loss': 0.3858, 'grad_norm': 1.8620983362197876, 'learning_rate': 9.909041542221016e-06, 'epoch': 1.02}\n",
      "{'loss': 0.39, 'grad_norm': 1.2203867435455322, 'learning_rate': 9.569644311702417e-06, 'epoch': 1.09}\n",
      "{'loss': 0.385, 'grad_norm': 0.7095856666564941, 'learning_rate': 9.230247081183817e-06, 'epoch': 1.15}\n",
      "{'loss': 0.4055, 'grad_norm': 1.2690999507904053, 'learning_rate': 8.890849850665218e-06, 'epoch': 1.22}\n",
      "{'loss': 0.3861, 'grad_norm': 0.8758470416069031, 'learning_rate': 8.55145262014662e-06, 'epoch': 1.29}\n",
      "{'loss': 0.3662, 'grad_norm': 1.0896631479263306, 'learning_rate': 8.212055389628021e-06, 'epoch': 1.36}\n",
      "{'loss': 0.4029, 'grad_norm': 0.9754967093467712, 'learning_rate': 7.872658159109422e-06, 'epoch': 1.43}\n",
      "{'loss': 0.3852, 'grad_norm': 0.9658946394920349, 'learning_rate': 7.5332609285908225e-06, 'epoch': 1.49}\n",
      "{'loss': 0.3876, 'grad_norm': 0.7637955546379089, 'learning_rate': 7.193863698072223e-06, 'epoch': 1.56}\n",
      "{'loss': 0.3814, 'grad_norm': 0.7597703337669373, 'learning_rate': 6.854466467553625e-06, 'epoch': 1.63}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m\n\u001b[1;32m     28\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     29\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     30\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2221\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"no\",  # No evaluation during training\n",
    "    learning_rate=1.5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    do_eval=False\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(pred.strip() for pred in decoded_pred.split()) for decoded_pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.strip() for label in decoded_label.split()) for decoded_label in decoded_labels]\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_samsum[\"train\"],\n",
    "    eval_dataset=tokenized_samsum[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb95ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compute metrics\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = trainer.evaluate(tokenized_samsum[\"test\"])\n",
    "print(metrics)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./saved_model\")\n",
    "tokenizer.save_pretrained(\"./saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2e32292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f238bab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18a2854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
